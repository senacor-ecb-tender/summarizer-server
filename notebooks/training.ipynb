{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Training LED for summarization\n",
    "\n",
    "This document provided by Senacor Technologies demonstrates how the model used in the Proof of Concept (PoC) solution was trained and provides instructions on how to reproduce and adapt the approach.\n",
    "\n",
    "## Table Of Contents\n",
    "\n",
    "- [Requirements](#Requirements)\n",
    "- [Overview](#Overview)\n",
    "- [Step 1: Language Model Pretraining](#Step-1:-Language-Model-Pretraining)\n",
    "  - [Training Data for Pretraining](#Training-Data-for-Pretraining)\n",
    "  - [Text Infilling Data Collator](#Text-Infilling-Data-Collator)\n",
    "  - [Masked Language Model Training](#Masked-Language-Model-Training)\n",
    "- [Step 2: Summarization Finetuning](#Step-2:-Summarization-Finetuning)\n",
    "  - [Training Data for Finetuning](#Training-Data-for-Finetuning)\n",
    "  - [Summarization Training](#Summarization-Training)\n",
    "- [Generating a Summary](#Generating-a-Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook assumes an environment with Python 3.8.10 and the following third-party dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.9.0 transformers==4.6.1 datasets==1.8.0 rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "As described in the provided documentation, the summarization model trained for and utilized in the PoC solution is a _Longformer-Encoder-Decoder_ (`LED`) as introduced in [_Longformer: The Long-Document Transformer_](https://arxiv.org/pdf/2004.05150.pdf) by Beltagy et al. of [Allen AI](https://allenai.org/).\n",
    "\n",
    "We use the implementation of `LED` provided as part of [`ðŸ¤— Transformers`](https://huggingface.co/transformers/index.html). Of the three checkpoints published togther with aforementioned paper, we found through experimentation that `led-large-16384-arxiv` is the most promising starting point. This checkpoint is pretrained on [the arXiv dataset](https://huggingface.co/datasets/scientific_papers) for long-document summarization and, as reported in the paper, exhibits state-of-the-art performance on this dataset.\n",
    "\n",
    "To adapt this model to the PoC objective, we found that the following two-step approach yields the best results:\n",
    "\n",
    "First, to adapt the model's language to the ECB's domain, we perform an additional short language model pretraining on texts published by the ECB. While we find that long language model pretraining negatively impacts the summarization performance, a relatively short masked language model training greatly improves the quality of the text generated by the final model.\n",
    "\n",
    "Second, we use a relatively small dataset of manually created summaries of selected ECB publications to finetune the model directly on the summarization objective.\n",
    "\n",
    "In the following, these two steps are described in more detail, including the fully functional code to recreate the training steps that we performed to obtain the model version used in the PoC solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Language Model Pretraining\n",
    "\n",
    "As the first step, we perform an additional short language model pretraining on the `led-large-16384-arxiv` checkpoint provided by _Allen AI_ to adapt it to the ECB's domain.\n",
    "\n",
    "\n",
    "### Training Data for Pretraining\n",
    "Self-supervised language model training requires a body of representative text. In the following, this notebook assumes that such a dataset is stored locally as a [`ðŸ¤— Dataset`](https://huggingface.co/docs/datasets/). It is assumed that this dataset contains both a `train` and an `eval` split.\n",
    "\n",
    "For the PoC, we obtained the training data by crawling [the ECB's website](https://www.ecb.europa.eu/home/html/index.en.html) for publicly available text documents, sanitizing the texts and pre-processing it using the tokenizer provided by _Allen AI_ with the `led-large-16384-arxiv` checkpoint.\n",
    "\n",
    "### Text Infilling Data Collator\n",
    "\n",
    "Since `LED` is based on `BART`, we would like to use the text infilling task for language model training, which has been shown to be a major contribution to `BART`'s outstanding performance on summarization tasks in the pretraining experiments reported in [the original `BART` paper](https://arxiv.org/pdf/1910.13461.pdf) by Lewis et al. of _Facebook AI_.\n",
    "\n",
    "There is currently no text infilling data collator in [`ðŸ¤— Transformers`](https://huggingface.co/transformers/index.html), so we provide our own. The implementation borrows ideas from `fairseq`'s more complex\n",
    "    [DenoisingDataset](https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/fairseq/data/denoising_dataset.py). It is likely that a further refined version of this collator will become part of `ðŸ¤— Transformers` in an upcoming release ([PR 12370](https://github.com/huggingface/transformers/pull/12370))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers.data.data_collator import _collate_batch\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForTextInfilling:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm_probability: float = 0.15\n",
    "    poisson_lambda: float = 3.0\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError\n",
    "\n",
    "    def __call__(self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
    "                 ) -> Dict[str, torch.Tensor]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\"input_ids\": _collate_batch(examples,\n",
    "                                                 self.tokenizer,\n",
    "                                                 pad_to_multiple_of=self.pad_to_multiple_of)}\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "\n",
    "        batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n",
    "            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def mask_tokens(self,\n",
    "                    inputs: torch.Tensor,\n",
    "                    special_tokens_mask: Optional[torch.Tensor] = None\n",
    "                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        labels = inputs.clone()\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "\n",
    "        # determine how many tokens we need to mask in total\n",
    "        is_token = ~(inputs == self.tokenizer.pad_token_id) & ~special_tokens_mask\n",
    "        num_to_mask = int(math.ceil(is_token.float().sum() * self.mlm_probability))\n",
    "\n",
    "        if num_to_mask == 0:\n",
    "            return inputs, labels\n",
    "\n",
    "        # generate a sufficient number of span lengths\n",
    "        poisson_distribution = torch.distributions.Poisson(rate=self.poisson_lambda)\n",
    "        lengths = poisson_distribution.sample(sample_shape=(num_to_mask,))\n",
    "        while torch.cumsum(lengths, 0)[-1] < num_to_mask:\n",
    "            lengths = torch.cat([lengths, poisson_distribution.sample(sample_shape=(num_to_mask,))])\n",
    "\n",
    "        # remove all spans of length 0\n",
    "        # Note that BART inserts additional mask tokens where length == 0,\n",
    "        # which we do not implement for now as it adds additional complexity\n",
    "        lengths = lengths[lengths > 0]\n",
    "\n",
    "        # trim to about num_to_mask tokens\n",
    "        idx = torch.argmin(torch.abs(torch.cumsum(lengths, 0) - num_to_mask)) + 1\n",
    "        lengths = lengths[:idx + 1]\n",
    "\n",
    "        # select span start indices\n",
    "        token_indices = is_token.nonzero(as_tuple=False)\n",
    "        span_starts = torch.randperm(token_indices.shape[0])[:lengths.shape[0]]\n",
    "\n",
    "        # prepare mask\n",
    "        masked_indices = token_indices[span_starts]\n",
    "        mask = torch.full_like(inputs, fill_value=False)\n",
    "\n",
    "        # mask span start indices\n",
    "        for mi in masked_indices:\n",
    "            mask[tuple(mi)] = True\n",
    "        lengths -= 1\n",
    "\n",
    "        # fill up spans\n",
    "        max_index = inputs.shape[1] - 1\n",
    "        remaining = (lengths > 0) & (masked_indices[:, 1] < max_index)\n",
    "        while torch.any(remaining):\n",
    "            masked_indices[remaining, 1] += 1\n",
    "            for mi in masked_indices:\n",
    "                mask[tuple(mi)] = True\n",
    "            lengths -= 1\n",
    "            remaining = (lengths > 0) & (masked_indices[:, 1] < max_index)\n",
    "\n",
    "        # place the mask tokens\n",
    "        mask[special_tokens_mask] = False\n",
    "        inputs[mask.bool()] = self.tokenizer.mask_token_id\n",
    "        labels[~mask.bool()] = -100\n",
    "\n",
    "        # remove mask tokens that are not starts of spans\n",
    "        to_remove = mask.bool() & mask.bool().roll(1, 1)\n",
    "        new_inputs = torch.full_like(inputs, fill_value=self.tokenizer.pad_token_id)\n",
    "        for i, example in enumerate(torch.split(inputs, split_size_or_sections=1, dim=0)):\n",
    "            new_example = example[0][~to_remove[i]]\n",
    "            new_inputs[i, 0:new_example.shape[0]] = new_example\n",
    "\n",
    "        return new_inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model Training\n",
    "\n",
    "The masked language model training is based on the training utilities provided in [`\n",
    "ðŸ¤— Transformers`](https://huggingface.co/transformers/index.html).\n",
    "\n",
    "The following is a fully functional but greatly simplified version of the training script used to generate the model version used in the proof of concept, where training was performed on a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "def mlm(batch_size: int, num_train_epochs: float, warmup_steps: int, learning_rate: float,\n",
    "        pretrained_model_name: str, dataset_path: str, \n",
    "        new_model_name: str):\n",
    "\n",
    "    # load training/validation dataset\n",
    "    ds = load_from_disk(dataset_path)\n",
    "\n",
    "    # initialize model and tokenizer\n",
    "    pretrained_model = LEDForConditionalGeneration.from_pretrained(pretrained_model_name,\n",
    "                                                                   gradient_checkpointing=True)\n",
    "\n",
    "    pretrained_model.led.num_beams = 1  # ensure that we use greedy search in generation\n",
    "    \n",
    "    tokenizer = LEDTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    # initialize the data collator\n",
    "    dc = DataCollatorForTextInfilling(tokenizer=tokenizer,\n",
    "                                      mlm_probability=.15,\n",
    "                                      pad_to_multiple_of=8)\n",
    "    \n",
    "    # initialize the trainer\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        output_dir=\"./mlm_outputs\",\n",
    "        overwrite_output_dir=True\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=pretrained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"eval\"],\n",
    "        data_collator=dc,\n",
    "        args=training_args\n",
    "    )\n",
    "   \n",
    "    # perform training\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "\n",
    "    # store the model\n",
    "    trainer.save_model(f\"./{new_model_name}\")\n",
    "\n",
    "    # evaluate the model\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use perform masked language model training on half of the about 1600 samples in our pretraining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm(batch_size=8, num_train_epochs=0.5, warmup_steps=80, learning_rate=5e-7,\n",
    "    pretrained_model_name=\"allenai/led-large-16384-arxiv\",\n",
    "    dataset_path=\"./ecb-publications\",\n",
    "    new_model_name=\"led-ecb-lm-arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Summarization Finetuning\n",
    "\n",
    "As the second step, we perform summarization finetuning on the pretrained model.\n",
    "\n",
    "### Training Data for Finetuning\n",
    "\n",
    "Summarization finetuning requires a selection of summaries for representative texts. In the following, this notebook assumes that such a dataset is stored locally as a [`ðŸ¤— Dataset`](https://huggingface.co/docs/datasets/). It is assumed that this dataset contains both a `train` and an `eval` split.\n",
    "\n",
    "For the PoC, we created the training data by hand-crafting summaries of different lengths and with different topical focus for selected publicly available ECB publications. The texts and summaries were then pre-processed for `LED` summarization finetuning using the tokenizer provided by _Allen AI_ with the `led-large-16384-arxiv` checkpoint.\n",
    "\n",
    "\n",
    "### Summarization Training\n",
    "\n",
    "The summarization training is based on the training utilities provided in [`ðŸ¤— Transformers`](https://huggingface.co/transformers/index.html).\n",
    "\n",
    "The following is a fully functional but greatly simplified version of the training script used to generate the model version used in the proof of concept, where training was performed on a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_from_disk, load_metric\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def _create_compute_metrics(tokenizer: LEDTokenizer):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels_ids = pred.label_ids\n",
    "        pred_ids = pred.predictions\n",
    "\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "        )[\"rouge2\"].mid\n",
    "\n",
    "        return {\n",
    "            \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "            \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "            \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        }\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def summarization(batch_size: int, num_train_epochs: float, warmup_steps: int, learning_rate: float,\n",
    "                  pretrained_model_path: str, dataset_path: str, new_model_name: str):\n",
    "    \n",
    "    # load training/validation dataset\n",
    "    ds = load_from_disk(dataset_path)\n",
    "\n",
    "    # initialize model and tokenizer\n",
    "    pretrained_model = LEDForConditionalGeneration.from_pretrained(pretrained_model_path,\n",
    "                                                                   gradient_checkpointing=True,\n",
    "                                                                   use_cache=False)\n",
    "    \n",
    "    tokenizer = LEDTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "    # set generation hyperparameters for training\n",
    "    pretrained_model.config.num_beams = 2\n",
    "    pretrained_model.config.max_length = 512\n",
    "    pretrained_model.config.min_length = 100\n",
    "    pretrained_model.config.length_penalty = 2.0\n",
    "    pretrained_model.config.early_stopping = True\n",
    "    pretrained_model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "    # initialize the trainer\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        output_dir=\"./summarization_outputs\",\n",
    "        overwrite_output_dir=True\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=pretrained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        compute_metrics=_create_compute_metrics(tokenizer),\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"eval\"]\n",
    "    )\n",
    "\n",
    "    # perform training\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "\n",
    "    # store the model\n",
    "    trainer.save_model(f\"./{new_model_name}\")\n",
    "\n",
    "    # evaluate the model\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform finetuning for 20 epochs on the about 80 documents in our summarization dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization(batch_size=2, num_train_epochs=20, warmup_steps=16, learning_rate=5e-5,\n",
    "              pretrained_model_path=\"./led-ecb-lm-arxiv\", dataset_path=\"./ecb-summaries\",\n",
    "              new_model_name=\"led-ecb-lm-arxiv-sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the context of the PoC we do not distinguish between short or long summaries in training. Finetuning the model with different generation settings for short and long summaries, or hinting to the model which length of summary is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Summary\n",
    "\n",
    "Now that the model is trained and stored, we can use it to generate summaries of long texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "\n",
    "trained_model = LEDForConditionalGeneration.from_pretrained(\"./led-ecb-lm-arxiv-sum\")\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"./led-ecb-lm-arxiv-sum\")\n",
    "\n",
    "beam_search_args = {\n",
    "    \"num_beams\": 2,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True\n",
    "}\n",
    "\n",
    "def summarize(text: str, length: str) -> str:\n",
    "    if length == \"short\":\n",
    "        min_length, max_length = 80, 200\n",
    "    elif length == \"long\":\n",
    "        min_length, max_length = 350, 600\n",
    "    else:\n",
    "        raise ValueError(f\"Length has to be either 'long' or 'short', not {length}.\")\n",
    "        \n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Global attention on the first token (cf. Beltagy et al. 2020)\n",
    "    global_attention_mask = torch.zeros_like(inputs)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask,\n",
    "                                 min_length=min_length, max_length=max_length, **beam_search_args)\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.txt\", \"rt\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(summarize(text, \"short\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "bleeding"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
